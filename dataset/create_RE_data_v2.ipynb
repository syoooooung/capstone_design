{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZTLRH3mDXO4eQMoPP0rco",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syoooooung/capstone_design/blob/main/dataset/create_RE_data_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai ragas datasets faiss-cpu groq langchain_community evaluate anls sentence_transformers\n",
        "!pip install -U langchain-openai\n",
        "import json\n",
        "from itertools import combinations\n",
        "import openai\n",
        "from google.colab import drive, userdata\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2fnefm_-yI2",
        "outputId": "5c9f576b-dedd-4f69-c04a-10670b07976b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Collecting ragas\n",
            "  Downloading ragas-0.2.6-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting groq\n",
            "  Downloading groq-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting anls\n",
            "  Downloading anls-0.0.2-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Collecting tiktoken (from ragas)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchain-openai (from ragas)\n",
            "  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
            "Collecting appdirs (from ragas)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pysbd>=0.3.4 (from ragas)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain)\n",
            "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Downloading ragas-0.2.6-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.12.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.8-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anls-0.0.2-py3-none-any.whl (12 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading langchain_openai-0.2.10-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: appdirs, xxhash, SQLAlchemy, python-dotenv, pysbd, mypy-extensions, marshmallow, httpx-sse, fsspec, faiss-cpu, dill, anls, typing-inspect, tiktoken, multiprocess, pydantic-settings, groq, dataclasses-json, langchain-core, langchain-openai, datasets, langchain, evaluate, langchain_community, ragas\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-2.0.35 anls-0.0.2 appdirs-1.4.4 dataclasses-json-0.6.7 datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 faiss-cpu-1.9.0.post1 fsspec-2024.9.0 groq-0.12.0 httpx-sse-0.4.0 langchain-0.3.8 langchain-core-0.3.21 langchain-openai-0.2.10 langchain_community-0.3.8 marshmallow-3.23.1 multiprocess-0.70.16 mypy-extensions-1.0.0 pydantic-settings-2.6.1 pysbd-0.3.4 python-dotenv-1.0.1 ragas-0.2.6 tiktoken-0.8.0 typing-inspect-0.9.0 xxhash-3.5.0\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.21)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (0.1.143)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.21->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.21->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY2')"
      ],
      "metadata": {
        "id": "Ur8zn4Th-3_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es8yxzKB-tHl",
        "outputId": "7fd8d44e-c4fa-4fd2-c278-a8e34ad4638a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "질문이 /content/questions.json 파일에 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import itertools\n",
        "\n",
        "# 입력 파일 경로와 출력 파일 경로\n",
        "input_file = \"/content/keyword_en.txt\"  # keyword_en.txt 파일 경로\n",
        "output_file = \"/content/questions.json\"  # 저장할 JSON 파일 경로\n",
        "\n",
        "# 데이터 로드 및 파싱\n",
        "data = []\n",
        "with open(input_file, \"r\") as file:\n",
        "    for line in file:\n",
        "        line = line.strip()\n",
        "        if line:  # 빈 줄 무시\n",
        "            parts = line.split(\" | \")\n",
        "            if len(parts) == 4:  # 형식에 맞는 데이터만 처리\n",
        "                data.append({\"A\": parts[0], \"B\": parts[1], \"C\": parts[2], \"D\": parts[3].strip(\")\")})  # \")\" 제거\n",
        "\n",
        "# D 기준으로 데이터 그룹화\n",
        "grouped_data = {}\n",
        "for item in data:\n",
        "    d_value = item[\"D\"]\n",
        "    if d_value not in grouped_data:\n",
        "        grouped_data[d_value] = []\n",
        "    grouped_data[d_value].append(item[\"C\"])\n",
        "\n",
        "# 질문 생성\n",
        "questions = []\n",
        "for d_value, c_list in grouped_data.items():\n",
        "    if len(c_list) < 3:\n",
        "        continue  # 3개 미만이면 조합을 만들 수 없음\n",
        "    for combination in itertools.combinations(c_list, 3):\n",
        "        question = f\"What is the {d_value} of {combination[0]} and {combination[1]} and {combination[2]}?\"\n",
        "        questions.append({\"combination\": {\"D\": d_value, \"C_values\": list(combination)}, \"question\": question})\n",
        "\n",
        "# JSON 파일로 저장\n",
        "with open(output_file, \"w\") as json_file:\n",
        "    json.dump(questions, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"질문이 {output_file} 파일에 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "combination에서 첫번째가 몇번씩 나왔는지"
      ],
      "metadata": {
        "id": "Jxr5nzTgLyg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# JSON 파일 경로\n",
        "file_path = '/content/reordered_questions.json'  # JSON 파일 경로에 맞게 수정\n",
        "\n",
        "# JSON 파일 로드\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# 첫 번째 요소 추출 및 카운트\n",
        "first_elements = [\n",
        "    item['combination']['C_values'][0]\n",
        "    for item in data\n",
        "    if 'combination' in item and 'C_values' in item['combination'] and item['combination']['C_values']\n",
        "]\n",
        "\n",
        "# 등장 횟수 카운트\n",
        "element_counts = Counter(first_elements)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"First elements in 'C_values' and their counts:\\n\")\n",
        "for element, count in element_counts.items():\n",
        "    print(f\"{element}: {count} times\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmZgYwmPL2Io",
        "outputId": "24445aef-62c2-40df-c213-54442df32540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First elements in 'C_values' and their counts:\n",
            "\n",
            "Neil Armstrong: 192 times\n",
            "Taylor Swift: 244 times\n",
            "Vladimir Putin: 241 times\n",
            "Donald Trump: 207 times\n",
            "Johnny Depp: 248 times\n",
            "Emma Watson: 158 times\n",
            "Elon Musk: 120 times\n",
            "Barack Obama: 46 times\n",
            "The film Titanic: 3 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "combination 편향될 인물로 위치 변경"
      ],
      "metadata": {
        "id": "rxuS3Y1qRR76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 파일 경로 설정\n",
        "input_file = \"/content/questions.json\"  # 기존 questions.json 파일\n",
        "output_file = \"/content/reordered_questions.json\"  # 재배치 후 저장할 파일\n",
        "\n",
        "# 주어진 리스트\n",
        "priority_list = [\n",
        "    \"Neil Armstrong\",\n",
        "    \"Vladimir Putin\",\n",
        "    \"Taylor Swift\",\n",
        "    \"Donald Trump\",\n",
        "    \"Elon Musk\",\n",
        "    \"Emma Watson\",\n",
        "    \"Johnny Depp\",\n",
        "    \"Barack Obama\",\n",
        "    \"The film Titanic\"\n",
        "]\n",
        "\n",
        "# JSON 파일 로드\n",
        "with open(input_file, \"r\") as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "# 질문 재배치 및 생성\n",
        "reordered_questions = []\n",
        "for entry in questions:\n",
        "    combination = entry[\"combination\"]\n",
        "    d_value = combination[\"D\"]\n",
        "    c_values = combination[\"C_values\"]\n",
        "\n",
        "    # 첫 번째, 두 번째, 세 번째 값 중 하나라도 priority_list에 있는지 확인\n",
        "    if not any(val in priority_list for val in c_values[:3]):\n",
        "        # 모두 없으면 건너뛰기\n",
        "        continue\n",
        "\n",
        "    # 첫 번째 값이 priority_list에 없는 경우\n",
        "    if c_values[0] not in priority_list:\n",
        "        # 두 번째와 세 번째 값 중 priority_list에 있는 값 찾기\n",
        "        found_priority = next((val for val in c_values[1:] if val in priority_list), None)\n",
        "        if found_priority:\n",
        "            # 해당 값을 첫 번째로 재배치\n",
        "            c_values.remove(found_priority)\n",
        "            c_values.insert(0, found_priority)\n",
        "\n",
        "    # 새로운 질문 생성\n",
        "    question = f\"What is the {d_value} of {c_values[0]} and {c_values[1]} and {c_values[2]}?\"\n",
        "    reordered_questions.append({\n",
        "        \"combination\": {\n",
        "            \"D\": d_value,\n",
        "            \"C_values\": c_values\n",
        "        },\n",
        "        \"question\": question\n",
        "    })\n",
        "\n",
        "# 결과 저장\n",
        "with open(output_file, \"w\") as file:\n",
        "    json.dump(reordered_questions, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"재배치된 질문이 {output_file} 파일에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx_NkeybRQcd",
        "outputId": "92d0afbc-2c1e-45d1-ed37-37145742cbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "재배치된 질문이 /content/reordered_questions.json 파일에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 파일 경로 설정\n",
        "input_file = \"/content/reordered_questions.json\"  # 기존 reordered_questions.json 파일\n",
        "output_file = \"/content/updated_questions.json\"  # 새로운 질문 저장할 파일\n",
        "\n",
        "# JSON 파일 로드\n",
        "with open(input_file, \"r\") as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "# 새로운 질문 생성\n",
        "updated_questions = []\n",
        "for entry in questions:\n",
        "    combination = entry[\"combination\"]\n",
        "    d_value = combination[\"D\"]\n",
        "    c_values = combination[\"C_values\"]\n",
        "\n",
        "    # 각 순서에 따라 새로운 질문 생성\n",
        "    for order, prefix in zip([\"first\", \"second\", \"last\"],\n",
        "                              [\"Who is the first\", \"Who is the second\", \"Who is the last\"]):\n",
        "        new_question = f\"{prefix} {d_value} of {c_values[0]} and {c_values[1]} and {c_values[2]}?\"\n",
        "        updated_questions.append({\n",
        "            \"combination\": combination,\n",
        "            \"question\": new_question,\n",
        "            \"order\": order\n",
        "        })\n",
        "\n",
        "# 결과 저장\n",
        "with open(output_file, \"w\") as file:\n",
        "    json.dump(updated_questions, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"새로운 질문이 {output_file} 파일에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIEn0xGZT605",
        "outputId": "416c3c24-e967-4617-90f1-3e0da3461155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "새로운 질문이 /content/updated_questions.json 파일에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer추가"
      ],
      "metadata": {
        "id": "fZMVZ9PpZLGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 파일 경로 설정\n",
        "input_file = \"/content/updated_questions.json\"  # 기존 reordered_questions.json 파일\n",
        "output_file = \"/content/updated_questions_with_answers.json\"  # 결과 저장 파일\n",
        "keyword_file = \"/content/keyword_en.txt\"  # keyword_en.txt 파일 경로\n",
        "\n",
        "# keyword_en.txt 로드\n",
        "keywords = []\n",
        "with open(keyword_file, \"r\") as file:\n",
        "    for line in file:\n",
        "        parts = line.strip(\"()\\n\").split(\" | \")\n",
        "        if len(parts) == 4:\n",
        "            keywords.append({\n",
        "                \"A\": parts[0],\n",
        "                \"B\": parts[1],  # B 값을 문자열로 저장\n",
        "                \"C\": parts[2],\n",
        "                \"D\": parts[3]\n",
        "            })\n",
        "\n",
        "# reordered_questions.json 로드\n",
        "with open(input_file, \"r\") as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "# answer 생성\n",
        "updated_questions = []\n",
        "for entry in questions:\n",
        "    combination = entry[\"combination\"]\n",
        "    d_value = combination[\"D\"]\n",
        "    c_values = combination[\"C_values\"]\n",
        "    order = entry.get(\"order\")\n",
        "\n",
        "    # D와 C에 맞는 B 값 필터링\n",
        "    matching_values = [\n",
        "        keyword[\"B\"] for keyword in keywords\n",
        "        if keyword[\"D\"] == d_value and keyword[\"C\"] in c_values\n",
        "    ]\n",
        "\n",
        "    # B 값을 정렬하여 적절한 answer 선택\n",
        "    matching_values.sort()  # 문자열로 정렬\n",
        "    if order == \"first\" and matching_values:\n",
        "        answer = matching_values[0]  # 가장 작은 값\n",
        "    elif order == \"second\" and len(matching_values) > 1:\n",
        "        answer = matching_values[1]  # 두 번째로 작은 값\n",
        "    elif order == \"last\" and matching_values:\n",
        "        answer = matching_values[-1]  # 가장 큰 값\n",
        "    else:\n",
        "        answer = None  # 매칭되는 값이 없는 경우\n",
        "\n",
        "    # 업데이트된 질문 생성\n",
        "    updated_entry = {\n",
        "        \"combination\": combination,\n",
        "        \"question\": entry[\"question\"],\n",
        "        \"gt_ans\": answer\n",
        "    }\n",
        "    updated_questions.append(updated_entry)\n",
        "\n",
        "# 결과 저장\n",
        "with open(output_file, \"w\") as file:\n",
        "    json.dump(updated_questions, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"새로운 질문과 answer가 {output_file} 파일에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UrAzLvBZMea",
        "outputId": "a314a6ff-8dcb-4f12-d69b-452ca3b336c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "새로운 질문과 answer가 /content/updated_questions_with_answers.json 파일에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UoPuBC_TdFbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM 부가설명"
      ],
      "metadata": {
        "id": "5xih7FZydFrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_prompt = \"\"\"\n",
        "Can you create a request by adding an additional explanation of {A} in this request?\n",
        "Keep the existing request form as much as possible, but add only the explanation of {A} in the middle.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KEtDFvv3gyR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import openai\n",
        "\n",
        "# LLM을 이용한 부가 설명 추가 함수\n",
        "def get_additional_info_for_keyword(question, keyword, model=\"gpt-4o\"):\n",
        "    # LLM을 통해 주어진 키워드에 대해 부가 설명을 요청하는 프롬프트 생성\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Addition keyword: {keyword}\n",
        "    Can you create a question by adding an additional explanation of {keyword} in this Question?\n",
        "    Keep the existing Question form as much as possible, but add only the explanation of {keyword} in the middle.\n",
        "\n",
        "    Ex)\n",
        "    Question: Who is the first birth date of Thomas Edison and Neil Armstrong and Vladimir Putin?\n",
        "    Addition keyword: Thomas Edison\n",
        "    ->\n",
        "    Who is the first birth date of Thomas Edison, a prolific inventor whose groundbreaking creations such as the electric light bulb, phonograph, and motion picture camera revolutionized modern life and whose relentless pursuit of innovation earned him the title of the 'Wizard of Menlo Park,' Neil Armstrong, and Vladimir Putin?\n",
        "\n",
        "    Just answer the question sentence and don't give any answer\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # LLM 응답에서 추가 설명 부분만 추출\n",
        "    additional_info = response.choices[0].message.content.strip()\n",
        "    return additional_info\n",
        "\n",
        "# 파일 경로 설정\n",
        "input_file = \"/content/updated_questions_with_answers.json\"  # 기존 업데이트된 질문 파일\n",
        "output_file = \"/content/updated_questions_with_descriptions.json\"  # 설명 추가된 결과 저장할 파일\n",
        "\n",
        "# reordered_questions.json 로드\n",
        "with open(input_file, \"r\") as file:\n",
        "    questions = json.load(file)\n",
        "\n",
        "# 각 question을 수정하여 새로운 질문 생성\n",
        "updated_questions = []\n",
        "tmp=0\n",
        "for entry in questions:\n",
        "    combination = entry[\"combination\"]\n",
        "    c_values = combination[\"C_values\"]\n",
        "\n",
        "    # 첫 번째 C_value에 대해서만 부가 설명 요청\n",
        "    keyword = c_values[0]\n",
        "    question = entry[\"question\"]\n",
        "    additional_info = get_additional_info_for_keyword(question, keyword)\n",
        "\n",
        "    # 기존 question에서 첫 번째 C_value와 부가 설명을 합쳐서 새로운 질문 만들기\n",
        "\n",
        "\n",
        "\n",
        "    # 수정된 질문 업데이트\n",
        "    entry[\"question\"] = additional_info\n",
        "    updated_questions.append(entry)\n",
        "\n",
        "# 결과 저장\n",
        "with open(output_file, \"w\") as file:\n",
        "    json.dump(updated_questions, file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"부가 설명이 추가된 질문이 {output_file} 파일에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWInGyHdHbU",
        "outputId": "8c585880-116e-44ba-c2b4-45483ff415cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "부가 설명이 추가된 질문이 /content/updated_questions_with_descriptions.json 파일에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07BULKiNh1GO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}