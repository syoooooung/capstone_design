{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP98g4FGk6ahHcggzq3a0nG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syoooooung/capstone_design/blob/main/Query_Decomposition/createSubQuery_by_conjunction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ8tVExduw3J",
        "outputId": "0188f88e-8ee8-4de6-f033-942d2529987d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.10/dist-packages (0.2.5)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: anls in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.17)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.142)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas) (0.8.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (from ragas) (0.2.8)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.6.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.17)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.1.142)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.17->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (4.66.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.54.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.17->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.17->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.17->langchain-openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai ragas datasets faiss-cpu groq langchain_community evaluate anls sentence_transformers\n",
        "!pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from google.colab import drive, userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('me_OPENAI_API_KEY')\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gS2ZqULu9y8",
        "outputId": "097d7a2a-9a83-47c7-f50b-c4ae2b195cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import openai\n",
        "\n",
        "# Load JSON data from file\n",
        "#input_path = \"/content/drive/MyDrive/Final/Dataset/MultiHop_RAG/multihop_test_v4.json\"\n",
        "input_path = \"/content/drive/MyDrive/Final/Dataset/HotpotQA/hotpotqa_test_v2.json\"\n",
        "output_path = \"/content/hotpot_QD_v6.json\"\n",
        "model = \"gpt-4o-mini\"  # Replace with your model if needed\n",
        "\n",
        "# 접속사와 콤마를 기준으로 분리하는 함수\n",
        "def split_sentence(text):\n",
        "    # 예외 처리할 접속사 패턴\n",
        "    exception_patterns = [\n",
        "#        r'\\b(?:both|either|neither|not only)\\b.*?\\b(?:and|or|nor)\\b',\n",
        "        r'\\b(?:not only)\\b.*?\\b(?:but also)\\b'\n",
        "    ]\n",
        "    # 예외 패턴에 해당하는 부분을 찾아서 미리 제외\n",
        "    for pattern in exception_patterns:\n",
        "        text = re.sub(pattern, lambda match: match.group(0).replace(\" \", \"_\"), text)  # 예외 접속사 사이의 공백을 \"_\"로 치환\n",
        "\n",
        "    # 일반적인 접속사와 콤마를 기준으로 분리\n",
        "    conjunctions = r'\\b(?:and|or|but|so|nor|yet)\\b'\n",
        "    parts = re.split(fr'\\s*(?:{conjunctions}|,)\\s*', text)\n",
        "\n",
        "    # \"_\"를 원래 공백으로 복구\n",
        "    parts = [part.replace(\"_\", \" \").strip() for part in parts if part.strip()]\n",
        "    return parts\n",
        "\n",
        "# 서브쿼리 생성 프롬프트\n",
        "def generate_subqueries2(original_query, split_results):\n",
        "    prompt = f\"\"\"\n",
        "Given a query with multiple entities or phrases separated by conjunctions or commas, create each subquery by keeping as much of the original phrasing as necessary to maintain the intent and meaning of the original query.\n",
        "\n",
        "Instructions:\n",
        "1. For each phrase or section after a conjunction or comma, create a complete subquery that retains necessary context from the original query.\n",
        "2. Ensure each subquery stands alone with full context so no information or intent is lost, and no subquery is overly simplified.\n",
        "3. Maintain original terms where possible and ensure each subquery is grammatically complete.\n",
        "When you fill in a sentence (subquery) in Separate Parts, always find it in the words in the original question and fill it in.\n",
        "\n",
        "\n",
        "Original Query: \"{original_query}\"\n",
        "Separated Parts:\n",
        "{split_results}\n",
        "\n",
        "Example:\n",
        "Original Query: \"Who has won more awards, Dan Schneider or Helen Hunt?\"\n",
        "->\n",
        "- subquery: Who has won more awards than Helen Hunt?\n",
        "- subquery: Who has won more awards than Dan Schneider?\n",
        "\n",
        "Example:\n",
        "Original Query: \"What do the Church of St. Mary of the Spring and Hagia Irene have in common?\"\n",
        "->\n",
        "- subquery: What do the Church of St. Mary of the Spring have in common?\n",
        "- subquery: What do Hagia Irene have in common?\n",
        "\n",
        "One more example:\n",
        "Original Query: \"Which band, Matthew Good Band or Switchfoot is still together?\"\n",
        "->\n",
        "- subquery: Which band, Matthew Good Band is still together?\n",
        "- subquery: Which band, Switchfoot is still together?\n",
        "\n",
        "Now, based on the above examples, provide a list of subqueries for each part of the question while preserving the essential meaning.\n",
        "- subquery: <- Don't say anything other than the format that starts with this form. And the decomposed query that is generated can't just generate the original question. Don't forget the purpose of dividing in the original question.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# LLM 호출 예시\n",
        "def get_subqueries2(original_query, model=\"gpt-4o-mini\"):\n",
        "    split_results = split_sentence(original_query)\n",
        "    prompt = generate_subqueries2(original_query, split_results)\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # 각 서브쿼리의 앞에 있는 \"subquery:\"와 \"-\"를 제거하고 리스트로 반환\n",
        "    subqueries = [\n",
        "        query.replace(\"- subquery: \", \"\").strip()  # \"subquery:\" 제거\n",
        "        for query in response.choices[0].message.content.splitlines() if query.strip()\n",
        "    ]\n",
        "    return subqueries\n",
        "\n",
        "# JSON 데이터 처리 함수\n",
        "def process_json(data):\n",
        "    processed_entries = []\n",
        "    tmp = 0\n",
        "    for entry in data:\n",
        "        tmp += 1\n",
        "        if tmp < 100:\n",
        "            continue\n",
        "\n",
        "        processed_entry = {\n",
        "            \"question\": entry[\"question\"],\n",
        "            \"ground_truth\": entry[\"answer\"],\n",
        "            \"ground_evidence\": entry.get(\"supporting_facts_sentences\", []),\n",
        "            \"decomposed_queries\": get_subqueries2(entry[\"question\"])\n",
        "        }\n",
        "        processed_entries.append(processed_entry)\n",
        "    return processed_entries\n",
        "\n",
        "# Load and process JSON\n",
        "with open(input_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "processed_data = process_json(data)\n",
        "\n",
        "# Save the updated JSON file\n",
        "with open(output_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(processed_data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"JSON file updated and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbCaqaaXu-QS",
        "outputId": "c5353890-e99c-44ae-9031-7a46eee13b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file updated and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "중복문장 제거"
      ],
      "metadata": {
        "id": "v549k5r31za5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# JSON 파일 로드\n",
        "with open('/content/hotpot_QD_v6.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# 중복된 decomposed_queries 제거 함수 정의\n",
        "def remove_exact_substrings(decomposed_queries):\n",
        "    unique_queries = decomposed_queries[:]\n",
        "\n",
        "    # 모든 쌍의 문장을 비교하여 짧은 문장의 마지막 한 글자 전까지가 긴 문장에 포함되는 경우 제거\n",
        "    for i in range(len(decomposed_queries)):\n",
        "        for j in range(i + 1, len(decomposed_queries)):\n",
        "            short, long = (decomposed_queries[i], decomposed_queries[j]) if len(decomposed_queries[i]) < len(decomposed_queries[j]) else (decomposed_queries[j], decomposed_queries[i])\n",
        "            # 짧은 문장의 마지막 한 글자 전까지가 긴 문장에 포함되는지 확인\n",
        "            if short[:-1] in long and short in unique_queries:\n",
        "                unique_queries.remove(short)\n",
        "\n",
        "    return unique_queries\n",
        "\n",
        "# 각 데이터 항목에 대해 중복 제거 적용\n",
        "for item in data:\n",
        "    item['decomposed_queries'] = remove_exact_substrings(item['decomposed_queries'])\n",
        "\n",
        "# 수정된 데이터를 새 JSON 파일로 저장\n",
        "with open('/content/hotpot_QD_v7.json', 'w') as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "print(\"중복이 제거된 decomposed_queries가 저장된 파일이 생성되었습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daQ4pAQb1ysN",
        "outputId": "4ef3c8aa-93c1-4539-c4fe-c6675abe82a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "중복이 제거된 decomposed_queries가 저장된 파일이 생성되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# FAISS 인덱스 파일 로드 (DB)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "file_path = '/content/drive/MyDrive/Jimin/MultihopRAG_vectorstore_250_v4.index'\n",
        "\n",
        "loaded_vectorstore = FAISS.load_local(\n",
        "    folder_path=file_path,\n",
        "    embeddings=embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "MO9n49r1vEbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever 설정 및 Top-k 설정 (하단)"
      ],
      "metadata": {
        "id": "yBFisugVvGQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = loaded_vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "XmUFNQY1vKbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "# Define LLM\n",
        "answer_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.1) # or gpt-4o-mini gpt-3.5-turbo\n",
        "\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "Please respond based only on the provided context, without using any prior knowledge.\n",
        "Don't answer in a sentence form. Just answer with the keyword of the answer and only just one word.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Setup RAG pipeline\n",
        "rag_chain = (\n",
        "    {\"context\": RunnablePassthrough(),  \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | answer_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "hKLe5wxivNe_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}